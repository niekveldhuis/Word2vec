{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akkadian Embeddings: Demonstration\n",
    "This is a demo for showing some potential avenues of research, using a word embedding model. The current notebook uses a vanilla word2vec model, but other models (such as Aleksi's PMI-based model) can easily be substituted. The final section demonstrates a way to visualize animal words as being more \"sheep-like\" or more \"horse-like\". This is inspired by a blog by [Ben Schmidt](http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html), characterizing food words from a corpus of recipes as more \"vegetable-like\" vs. more \"meat-like\". This blog post also goes a long way in explaining in a non-technical way the basics of what a word embedding model is (quite independent of the technique used for creating such a model) - highly recommended reading.\n",
    "\n",
    "The challenge will be to build upon these examples, find words or groups of words that provide interesting contrasts and relationships with other words.\n",
    "\n",
    "This notebook builds upon a notebook written by me and Laura Nelson (now at Northwestern University) for a class in Computational Text Analysis at UC Berkeley.\n",
    "\n",
    "# Corpus\n",
    "\n",
    "The corpus used for this notebook consists of the following ORACC projects, using only the Akkadian words.\n",
    "\n",
    "> 'adsd/adart1',\n",
    " 'adsd/adart2',\n",
    " 'adsd/adart3',\n",
    " 'adsd/adart6',\n",
    " 'aemw/amarna',\n",
    " 'aemw/idrimi',\n",
    " 'akklove',\n",
    " 'atae/assur',\n",
    " 'atae/burmarina',\n",
    " 'atae/durkatlimmu',\n",
    " 'atae/guzana',\n",
    " 'atae/huzirina',\n",
    " 'atae/imgurenlil',\n",
    " 'atae/kalhu',\n",
    " 'atae/mallanate',\n",
    " 'atae/marqasu',\n",
    " 'atae/nineveh',\n",
    " 'atae/samal',\n",
    " 'atae/tilbarsip',\n",
    " 'bbto',\n",
    " 'blms',\n",
    " 'cams/anzu',\n",
    " 'cams/barutu',\n",
    " 'cams/etana',\n",
    " 'cams/gkab',\n",
    " 'cams/ludlul',\n",
    " 'cams/selbi',\n",
    " 'caspo',\n",
    " 'caspo/akkpm',\n",
    " 'ccpo',\n",
    " 'cmawro/cmawr1',\n",
    " 'cmawro/cmawr2',\n",
    " 'cmawro/cmawr3',\n",
    " 'cmawro/maqlu',\n",
    " 'dcclt',\n",
    " 'dcclt/nineveh',\n",
    " 'dcclt/signlists',\n",
    " 'dccmt',\n",
    " 'glass',\n",
    " 'hbtin',\n",
    " 'riao',\n",
    " 'rinap/rinap1',\n",
    " 'rinap/rinap2',\n",
    " 'rinap/rinap3',\n",
    " 'rinap/rinap4',\n",
    " 'rinap/rinap5',\n",
    " 'saao/saa01',\n",
    " 'saao/saa02',\n",
    " 'saao/saa03',\n",
    " 'saao/saa04',\n",
    " 'saao/saa05',\n",
    " 'saao/saa06',\n",
    " 'saao/saa07',\n",
    " 'saao/saa08',\n",
    " 'saao/saa09',\n",
    " 'saao/saa10',\n",
    " 'saao/saa11',\n",
    " 'saao/saa12',\n",
    " 'saao/saa13',\n",
    " 'saao/saa14',\n",
    " 'saao/saa15',\n",
    " 'saao/saa16',\n",
    " 'saao/saa17',\n",
    " 'saao/saa18',\n",
    " 'saao/saa19',\n",
    " 'saao/saa20',\n",
    " 'saao/saa21',\n",
    " 'saao/saas2',\n",
    " 'suhu',\n",
    " 'tcma/ali1',\n",
    " 'tcma/assur',\n",
    " 'tcma/barri',\n",
    " 'tcma/bazmusian',\n",
    " 'tcma/billa',\n",
    " 'tcma/brak',\n",
    " 'tcma/chuera',\n",
    " 'tcma/emar',\n",
    " 'tcma/fekheriye',\n",
    " 'tcma/giricano',\n",
    " 'tcma/hana',\n",
    " 'tcma/haradum',\n",
    " 'tcma/hatti',\n",
    " 'tcma/kartn',\n",
    " 'tcma/kulishinas',\n",
    " 'tcma/laws',\n",
    " 'tcma/miscellaneous',\n",
    " 'tcma/nineveh',\n",
    " 'tcma/nippur',\n",
    " 'tcma/nuzi',\n",
    " 'tcma/qitar',\n",
    " 'tcma/rimah',\n",
    " 'tcma/suri',\n",
    " 'tcma/taban',\n",
    " 'tcma/tsa1',\n",
    " 'tcma/tsh1'\n",
    "\n",
    "It may well be that a smaller (more focused) corpus yields better results, in particular when using PMI-based models (that can deal with smaller corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform, cosine\n",
    "# Data Wrangling\n",
    "import os\n",
    "import gensim #library needed for word2vec\n",
    "#for visualization\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.manifold import MDS, TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output/parsed.csv',index_col=None, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many texts are included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data.shape\n",
    "#data.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#tokenize the data by splitting on white space. There is no punctuation in this text.\n",
    "data['tokens'] = data['lemma'].str.split()\n",
    "data['tokens'][0][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Unlemmatized (broken or unknown) words are represented as, for instance, `x-ši-ka[NA]NA`. Such tokens are essentially placeholders. One may try two different approaches:\n",
    "- represent all such placeholders by NA\n",
    "- eliminate all placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_NA = data.copy()\n",
    "for i in range(len(data_NA)):\n",
    "    data_NA['tokens'][i] = [token if not token.endswith('NA]NA') else 'NA' for token in data_NA['tokens'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['tokens'][i] = [token for token in data['tokens'][i] if not token.endswith('NA]NA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data['tokens'][0][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_NA['tokens'][0][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#fit a word2vec model on the tokenized data, with all the default options\n",
    "#setting the 'worker' option to 1 should ensure reproducibility\n",
    "#As per the docs of Gensim, for executing a fully deterministically-reproducible run, \n",
    "#you must also limit the model to a single worker thread, \n",
    "#to eliminate ordering jitter from OS thread scheduling.\n",
    "\n",
    "model = gensim.models.Word2Vec(data_NA['tokens'], size=100, window=5, \\\n",
    "                               min_count=1, sg=1, alpha=0.025, iter=5, batch_words=10000, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#view the 100 element vector for the word 'ēkallu[palace]N'\n",
    "#each token (not document) has a 100 element vector\n",
    "model.wv['ēkallu[palace]N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find cosine distance between two given word vectors\n",
    "model.wv.similarity('ēkallu[palace]N','bītu[house]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#find the 10 most similar vectors to the given word vector\n",
    "model.wv.most_similar('ēkallu[palace]N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paradigmatic example of what word2vec can do is the so-called analogy task: \n",
    "> man : woman = king : ?\n",
    "\n",
    "As it turns out, our model is not very good with that question (perhaps because kings appear so much more frequently than queens), but it does OK with some animal analogies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#For analogies, use both positive and negative vectors. The target word, in this case, is lamb.\n",
    "# if sheep - lamb = ox - calf, then ox = sheep + calf - lamb\n",
    "model.wv.most_similar(positive=['immeru[sheep]N', 'būru[(bull)-calf]N'], negative=['puhādu[lamb]N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# visualize the 50 most similar vectors to two words meaning 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(['lemnu[bad]AJ', 'masku[bad]AJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Graph for the fifty most similar tokens\n",
    "bad_tokens = [token for token,weight in model.wv.most_similar(['lemnu[bad]AJ', 'masku[bad]AJ'], topn=50)]\n",
    "vectors = [model.wv[word] for word in bad_tokens]\n",
    "dist = pdist(vectors, metric='cosine')\n",
    "dist_matrix = squareform(dist)\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=1, color='b')\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(bad_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try to find different senses of the word bad by removing the vector for 'an evil demon'\n",
    "model.wv.most_similar(positive=['masku[bad]AJ','lemnu[bad]AJ'], negative=['utukku[(an-evil-demon)]N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#remove more vectors to get at different senses of the word 'bad'\n",
    "model.wv.most_similar(positive=['masku[bad]AJ','lemnu[bad]AJ'], negative=['utukku[(an-evil-demon)]N','dipalû[distortion-of-justice]N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a list of animals and animal words.\n",
    "List animal words and words that are associated with these animals. One could also derive a list of animal words from [Ura 14](http://oracc.org/dcclt/Q000089) and [15](http://oracc.org/dcclt/Q000090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals = ['sisû[horse]N', 'immeru[sheep]N', 'imēru[donkey]N', 'alpu[ox]N', \n",
    "           'pīru[elephant]N', 'yābilu[ram]N', 'udru[Bactrian-camel]N', 'damdāmu[(a-kind-of-mule)]N'\n",
    "           ,'atānu[she-ass]N', 'būru[(bull)-calf]N', 'tuānu[(a-breed-of-horse)]N', 'agālu[donkey]N'\n",
    "          , 'šullāmu[(a-type-of-horse)]N', 'sugullu[herd]N', 'anāqāte[she-camels]N',\n",
    "          'gurrutu[ewe]N', 'irginu[(a-breed-or-colour-of-horse)]N', \n",
    "           'huzīru[pig]N', 'pēthallu[riding-horse]N', 'puhādu[lamb]N']\n",
    "animal_words = model.wv.most_similar(animals, topn=10)\n",
    "animal_words = [word for word, similarity in animal_words]\n",
    "animal_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horses and Sheep\n",
    "The animal vocabulary may be divided into 'horse-vocabulary' (used for war and often received from foreign countries) and sheep vocabulary. Sheep are domestic animals held for meat and wool and are (relatively) close to other such animals (ox, calf) and words that have to do with wool production. All animal words collected above are located on a graph - the X-axis represents the cosine similarity to \"sheep\" the Y-axis the cosine similarity to \"horse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "animals_assoc = animals + animal_words\n",
    "animals_assoc = list(set(animals_assoc)) # remove duplicates\n",
    "x = [model.wv.similarity('sisû[horse]N', word) for word in animals_assoc]\n",
    "y = [model.wv.similarity('immeru[sheep]N', word) for word in animals_assoc]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show\n",
    "from bokeh.models import (BoxSelectTool, Circle, HoverTool,\n",
    "                          Plot, BoxZoomTool, ResetTool, SaveTool)\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bokeh.models import TextInput, Button, ColumnDataSource, LabelSet\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'x_values' : x, 'y_values' : y, 'labels': animals_assoc}\n",
    "source = ColumnDataSource(data=data)\n",
    "p = figure(\n",
    "        plot_width=800, plot_height=1000,\n",
    "        tools=\"tap,pan,wheel_zoom,box_zoom,reset,save\")\n",
    "p.circle(x='x_values', y='y_values', source=source)\n",
    "p.add_tools(HoverTool(\n",
    "        tooltips=[\n",
    "            (\"\", \"@labels\"), \n",
    "            (\"sheep\", \"@y_values\"),\n",
    "            (\"horse\", \"@x_values\")]\n",
    "        ))\n",
    "labels = LabelSet(\n",
    "            x='x_values',\n",
    "            y='y_values',\n",
    "            text='labels',\n",
    "            level='glyph',\n",
    "            x_offset=5, \n",
    "            y_offset=5, \n",
    "            source=ColumnDataSource(data), \n",
    "            render_mode='canvas'\n",
    "            )\n",
    "p.add_layout(labels)\n",
    "p.line([0, 1], [0, 1], color = \"red\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bokeh plot above has various tools that may be selected with the symbols to the right of the graph: Zoom in, Save, Reset, Pan. It also uses tiptools to show the cosine similarity of a word with sheep and horse, respectively. A Bokeh graph can also use links to websites (an oracc edition, or a glossary entry, for instance).\n",
    "\n",
    "The words immeru\\[sheep\\]N and sisû\\[horse\\]N are to be found at (0.3167, 1) and (1, 0.3167), respectively, because immeru\\[sheep\\]N and sisû\\[horse\\]N have a cosine similarity of 0.3167 (in the current model). When compared to itself, cosine similarity is 1 (the maximum value).\n",
    "\n",
    "The red line (diagonal) separates the words that are more sheep-like from those that are more horse-like.\n",
    "\n",
    "Bokeh plots can also be saved as independent HTML files that can be displayed on a website, preserving the interactive tools. Such files are useful for presenting research results. I highly recommend using Bokeh or a similar visualization library that can create interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('immeru[sheep]N', 'sisû[horse]N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
