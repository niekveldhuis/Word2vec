{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lemmatization from ORACC JSON\n",
    "The code in this notebook will parse [ORACC](http://oracc.org) `JSON` files to extract lemmatization data for projects with Akkadian data. The resulting `csv` (Comma Separated Values) file is named `parsed.csv` and has two fields: a Text ID (e.g. `dcclt/Q000039`) and a string of lemmas in the format `Å¡arru[king]N`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import errno\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Create Directories, if Necessary\n",
    "The two directories needed for this script are `jsonzip` and `output`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(x):\n",
    "    \"\"\"Check for existence of directories\n",
    "    create those if they do not exist\n",
    "    otherwise do nothing. Parameter is a list\n",
    "    with directory names\"\"\"\n",
    "    for d in x:\n",
    "        try:\n",
    "            os.mkdir(d)\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input Project Names\n",
    "Provide a list of one or more project names, separated by commas. Note that subprojects must be listed separately, they are not included in the main project. For instance:\n",
    "\n",
    "`saao/saa01,saao/saa02,blms`\n",
    "\n",
    "The input is split into a proper list that python can iterate over using the `format_project_list()` function in the `utils` module. The code of this function is discussed in more detail in 2.1.0. Download ORACC JSON Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"\"\"saao/saa01, saao/saa02, saao/saa03, saao/saa04, saao/saa05, saao/saa06,\n",
    "           saao/saa07, saao/saa08, saao/saa09, saao/saa10, saao/saa11, saao/saa12,\n",
    "           saao/saa13, saao/saa14, saao/saa15, saao/saa16, saao/saa17, saao/saa18, saao/saa19,\n",
    "           saao/saa20, saao/saa21, saao/saas2,\n",
    "           rinap/rinap1, rinap/rinap2, rinap/rinap3, rinap/rinap4, rinap/rinap5,\n",
    "           riao,\n",
    "           atae/assur, atae/burmarina, atae/durkatlimmu, atae/guzana, atae/huzirina, atae/imgurenlil,\n",
    "           atae/kalhu, atae/mallanate, atae/marqasu, atae/nineveh, atae/samal, atae/tilbarsip,\n",
    "           tcma/assur, tcma/nineveh, tcma/kartn, tcma/ali1, tcma/giricano, tcma/tsh1, tcma/emar,\n",
    "           tcma/hana, tcma/haradum, tcma/chuera, tcma/barri, tcma/kulishinas, tcma/brak, tcma/nuzi, \n",
    "           tcma/rimah, tcma/qitar, tcma/taban, tcma/billa, tcma/suri, tcma/bazmusian, tcma/tsa1,\n",
    "           tcma/fekheriye, tcma/hatti, tcma/nippur, tcma/miscellaneous, tcma/laws,\n",
    "           cmawro/cmawr1, cmawro/cmawr2, cmawro/cmawr3, cmawro/maqlu,\n",
    "           dcclt, dcclt/nineveh, dcclt/signlists,\n",
    "           cams/gkab, cams/anzu, cams/barutu, cams/ludlul, cams/selbi, cams/etana,\n",
    "           hbtin,\n",
    "           adsd/adart1, adsd/adart2, adsd/adart3, adsd/adart6,\n",
    "           aemw/idrimi, aemw/amarna,\n",
    "           akklove,\n",
    "           bbto,\n",
    "           blms,\n",
    "           caspo, caspo/akkpm,\n",
    "           ccpo,\n",
    "           dccmt,\n",
    "           glass,\n",
    "           suhu\n",
    "           \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_project_list(x):\n",
    "    p = x.lower().strip().split(',')\n",
    "    p = [x.strip() for x in p]\n",
    "    p = list(set(p))\n",
    "    p.sort()\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = format_project_list(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Download the ZIP files.\n",
    "Download the zipped JSON files using the `oracc_download()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracc_download(p, server = 'penn'):\n",
    "    \"\"\"Downloads ZIP with JSON files from\n",
    "    ORACC servers. First parameter is a list\n",
    "    with ORACC project names,\n",
    "    return is the same list of names,\n",
    "    minus doublets and non-existing\n",
    "    projects. Second parameter is 'lmu' \n",
    "    (first try LMU server) or 'penn' \n",
    "    (default: first try Penn server).\"\"\"\n",
    "    \n",
    "    CHUNK = 1024\n",
    "    for project in p.copy():\n",
    "        proj = project.replace('/', '-')\n",
    "        build = f\"http://build-oracc.museum.upenn.edu/json/{proj}.zip\"\n",
    "        oracc = f\"http://oracc.org/{project}/json/{proj}.zip\"\n",
    "        lmu = f\"http://oracc.ub.uni-muenchen.de/{project}/json/{proj}.zip\"\n",
    "        file = f\"jsonzip/{proj}.zip\"\n",
    "        servers = [build, oracc, lmu]\n",
    "        if server == 'lmu':\n",
    "            servers = [lmu, build, oracc]\n",
    "        for url in servers:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                if r.status_code == 200:\n",
    "                    tqdm.write(f\"Saving {url} as {file}.\")\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    t=tqdm(total=total_size, unit='B', unit_scale=True, desc = project)\n",
    "                    with open(file, 'wb') as f:\n",
    "                        for c in r.iter_content(chunk_size=CHUNK):\n",
    "                            t.update(len(c))\n",
    "                            f.write(c)\n",
    "                    break\n",
    "                else:\n",
    "                    if url == servers[-1]: #last server in the list was tried\n",
    "                        print(f\"WARNING {url} does not exist.\")\n",
    "                        p.remove(project)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = oracc_download(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"head21\"></a>2.1 The `parsejson()` function\n",
    "The `parsejson()` function will \"dig into\" the `json` file (transformed into a dictionary) until it finds the relevant data. The `json` file consists of a hierarchy of `cdl` nodes; only the lowest nodes contain lemmatization data. The function goes down this hierarchy by calling itself when another `cdl` node is encountered. For more information about the data hierarchy in the [ORACC](http://oracc.org) `json` files, see [ORACC Open Data](http://oracc.org/doc/opendata/index.html).\n",
    "\n",
    "The first argument of the `parsejson()` function is a `JSON` object, essentially a Python dictionary that initially contains the entire contents of the original JSON file. The code takes the key `cdl` the value of which is a list of `JSON` dictionaries. Iterating through these dictionaries, if a dictionary contains another `cdl` node, the function calls itself with this lower-level dictionary as argument. This way the function digs deeper and deeper into the `JSON` tree, until it does not encounter a `cdl` key anymore. Here we are at the level of individual words. The code checks for a key `f`, if it exists the value of that key (another dictionary) is appended to the list `l`. The list `l` will become a list of dictionaries, where each dictionary represents a single word and the list represents the vocabulary of a single text with the words in their original order.\n",
    "\n",
    "The second argument of the function, `id_text`, consists of a project abbreviation, such as `blms` or `cams/gkab` plus a text ID, in the format `cams/gkab/P338616` or `dcclt/Q000039`. This ID is added to the lemmatization data of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text, id_text):\n",
    "    l = []\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            l.extend(parsejson(JSONobject, id_text))\n",
    "        if \"f\" in JSONobject:\n",
    "            lemm = JSONobject[\"f\"]\n",
    "            lemm[\"id_text\"] = id_text\n",
    "            l.append(lemm)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Call the `parsejson()` function for every `JSON` file\n",
    "The code in this cell will iterate through the list of projects entered above (1.1). For each project the `JSON` zip file is located in the directory `jsonzip`, named PROJECT.zip. The `zip` file contains a directory that is called `corpusjson` that contains a JSON file for every text that is available in that corpus. The files are called after their text IDs in the pattern `P######.json` (or `Q######.json` or `X######.json`).\n",
    "\n",
    "The function `namelist()` of the `zipfile` package is used to create a list of the names of all the files in the ZIP. From this list we select all the file names in the `corpusjson` directory with extension `.json` (this way we exclude the name of the directory itself). \n",
    "\n",
    "Each of these files is read from the `zip` file and loaded with the command `json.loads()`, which transforms the string into a proper JSON object. \n",
    "\n",
    "This JSON object (essentially a Python dictionary), which is called `data_json` is now sent to the `parsejson()` function. The function returns a list of lemmata (each lemma represented by a dictionary). The `extend()` method is used to add this list to the list `lemm_l`, one document at the time. In the end, `lemm_l` will contain as many list elements as there are words in all the texts in the projects requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = [] # initiate the list that will hold all the lemmatization data of all texts in all requested projects\n",
    "for project in p:\n",
    "    # print(\"Parsing \" + project)\n",
    "    file = f\"jsonzip/{project.replace('/', '-')}.zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(f\"{file} does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc=project):                            #iterate over the file names\n",
    "        id_text = project + filename[-13:-5] # id_text is, for instance, blms/P414332\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            lemm_l.extend(parsejson(data_json, id_text))               # and send to the parsejson() function\n",
    "        except:\n",
    "            tqdm.write(f'{id_text} is not available or not complete')\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Structuring\n",
    "### 3.1 Transform the Data into a DataFrame\n",
    "The list `lemm_l` is transformed into a `pandas` dataframe (`word_df`) for further manipulation. The `pandas` function `fillna('')` fills gaps in the dataframe. If a particular data field is not available for a particular row, `pandas` will fill that cell with NaN (\"Not a Number\"). The data type of NaN is numeric, which creates problems in string manipulation further down the road (for instance in creating a Lemma column, section 3.2). The function `fillna()` will fill all empty cells with the argument it is given - in this case the empty string.\n",
    "\n",
    "For various reasons not all JSON files will have all data types that potentially exist in an [ORACC](http://oracc.org) lemmatization. For instance, only Sumerian words have a `base`, so if your data set has no Sumerian, this column will not exist in the DataFrame. Where such fields are referenced below, the code may fail and you may need to adjust some lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(lemm_l)\n",
    "word_df = word_df.fillna('')      # replace NaN (Not a Number) with empty string\n",
    "word_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces and commas in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens and nothing (empty string), respectively. By default the `replace()` function in `pandas` will match the entire string (that is, \"lugal\" matches \"lugal\" but there is no match between \"l\" and \"lugal\"). In order to match partial strings the parameter `regex` must be set to `True`.\n",
    "\n",
    "The `replace()` function takes a nested dictionary as argument. The top-level keys identify the columns on which the `replace()` function should operate (in this case 'gw' and 'sense'). The value of each key is another dictionary with the search string as key and the replace string as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findreplace = {' ' : '-', ',' : ''}\n",
    "word_df = word_df.replace({'gw' : findreplace, 'sense' : findreplace}, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create a `lemma` column\n",
    "A lemma, [ORACC](http://oracc.org) style, combines Citation Form, GuideWord and POS into a unique reference to one particular lemma in a standard dictionary, as in `Å¡arru[king]N`. Usually, not all words in a text are lemmatized, because a word may be (partly) broken and/or unknown. Unlemmatized and unlemmatizable words will receive a place-holder lemmatization that consists of the transliteration of the word (instead of the Citation Form), with `NA` as GuideWord and `NA` as POS, as in `i-bu-x[NA]NA`. Note that `NA` is a string. Remove words that are not in Akkadian (for instance, in bilingual texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df[\"lemma\"] = word_df[\"cf\"] + '[' + word_df[\"gw\"] + ']' + word_df[\"pos\"]\n",
    "word_df.loc[word_df[\"cf\"] == \"\" , 'lemma'] = word_df['form'] + '[NA]NA'\n",
    "word_df = word_df.loc[word_df.lang.str.startswith('akk')]\n",
    "word_df[['id_text', 'lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Group by Textid\n",
    "Get all the lemmas that belong to a single text in one row (one row = one document). The `agg()` (aggregate) function, which works on the result of a `groupby()` process aggregates columns of the original dataframe. The aggregate function takes a dictionary in which the keys are column names and the values are functions to be used in the aggregation process. The example below has only one such function (`' '.join` will join all entries in the colum `lemma` with a space in between); one may specify (the same or different) functions for different columns, for instance:\n",
    "```python\n",
    "word_df = word_df.groupby(\"textid\").agg({\"lemma\": ' '.join, \"base\": ' '.join})\n",
    "```\n",
    "The process will create a compound index. It is useful to flatten the index with a reset (`reset_index()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = word_df.groupby(\"id_text\").agg({\"lemma\": ' '.join})\n",
    "doc_df = doc_df.reset_index()\n",
    "doc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Save the Results\n",
    "## 4.1 Save Results in CSV file\n",
    "The output file is called `parsed.csv` and is placed in the directory `output`. In most cases, `csv` files open automatically in Excel. This program does not deal well with `utf-8` encoding. If you intend to use the file in Excel, change `encoding ='utf-8'` to `encoding='utf-16'`. For usage in computational text analysis applications `utf-8` is usually preferred. The option `index=False` excludes the (numerical) index from the saved `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savefile =  'parsed.csv'\n",
    "with open(f'output/{savefile}', 'w', encoding=\"utf-8\") as w:\n",
    "    doc_df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Pickle\n",
    "The Pandas function `to_pickle()` writes a binary file that can be opened in a later phase of the project with the `read_pickle()` command and will reproduce exactly the same DataFrame. The resulting file cannot be used in other programs but preserves the data structure of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled = \"output/parsed.p\"\n",
    "doc_df.to_pickle(pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {
    "1a0765daf4e0472f83be298cf8808e03": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "40ef197148e94834ba2417c853c1b482": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "81fba4c42876419fbf5d56d9fc9a8d00": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "a625fb6a45fa4a2982743fa41093283f": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "cc18adf625a54216acf29f0c845e924a": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "e0f8646a68994a1dbf473cae8f8fc50e": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "f0e9a7ca11754be2bd4a6993cc88cdd7": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "fa9c8f869232467a960d39b62e6669d6": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
